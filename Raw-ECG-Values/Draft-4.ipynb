{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "566d29cd-c051-445b-a965-9bdd81b2e32f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\abbas\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wfdb\n",
    "import os\n",
    "import scipy.signal as sgn\n",
    "from tqdm import tqdm\n",
    "from keras import models, layers, optimizers, regularizers\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from scipy.signal import cheby1, filtfilt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "837e9718-0d1f-4aea-bec8-49663ea1933e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# File paths\n",
    "train_files = [f\"a{str(i).zfill(2)}\" for i in range(1, 21)] + [f\"b{str(i).zfill(2)}\" for i in range(1, 6)] + [f\"c{str(i).zfill(2)}\" for i in range(1, 11)]\n",
    "test_files = [f\"x{str(i).zfill(2)}\" for i in range(1, 36)]\n",
    "base_path = \"apnea-ecg/1.0.0/\"\n",
    "train_paths = [os.path.join(base_path, file) for file in train_files]\n",
    "test_paths = [os.path.join(base_path, file) for file in test_files]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dede400d-6537-4615-a578-474ebcefc295",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Chebyshev Filter\n",
    "def apply_chebyshev_filter(signal, lowcut=0.5, highcut=40, fs=100, order=4):\n",
    "    nyquist = 0.5 * fs\n",
    "    low = lowcut / nyquist\n",
    "    high = highcut / nyquist\n",
    "    b, a = cheby1(order, 0.5, [low, high], btype='band')\n",
    "    return filtfilt(b, a, signal)\n",
    "\n",
    "# Function to load and preprocess the ECG signal\n",
    "def load_ecg_and_segment(file, segment_duration=10, fs=100):\n",
    "    if not os.path.exists(f\"{file}.hea\") or not os.path.exists(f\"{file}.dat\"):\n",
    "        print(f\"File not found: {file}.hea or {file}.dat\")\n",
    "        return [], []\n",
    "    \n",
    "    record = wfdb.rdrecord(file)\n",
    "    annotation = wfdb.rdann(file, 'apn')\n",
    "\n",
    "    signal = record.p_signal[:, 0]\n",
    "    filtered_signal = apply_chebyshev_filter(signal)\n",
    "\n",
    "    segments = []\n",
    "    labels = []\n",
    "    samples_per_segment = segment_duration * fs\n",
    "\n",
    "    for i, samp in enumerate(annotation.sample):\n",
    "        start = max(samp - samples_per_segment // 2, 0)\n",
    "        end = start + samples_per_segment\n",
    "        if end > len(filtered_signal):\n",
    "            break\n",
    "        segment = filtered_signal[start:end]\n",
    "        segments.append(segment)\n",
    "        labels.append(1 if annotation.symbol[i] == 'A' else 0)\n",
    "        \n",
    "    return np.array(segments), np.array(labels)\n",
    "\n",
    "# Load and prepare the full dataset\n",
    "def prepare_data(paths):\n",
    "    data, labels = [], []\n",
    "    for path in tqdm(paths):\n",
    "        segments, seg_labels = load_ecg_and_segment(path)\n",
    "        if segments.size > 0:  \n",
    "            data.extend(segments)\n",
    "            labels.extend(seg_labels)\n",
    "    return np.array(data), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55da9faf-3c12-4234-842c-e747d976c979",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 35/35 [00:03<00:00, 10.13it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 35/35 [00:03<00:00,  9.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training segments: 13064, Test segments: 17268\n",
      "Training data shape: (13064, 1000, 1)\n",
      "Training labels shape: (13064,)\n",
      "Test data shape: (17268, 1000, 1)\n",
      "Test labels shape: (17268,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Load training and validation data, and shuffle\n",
    "X_train, y_train = prepare_data(train_paths)\n",
    "X_test, y_test = prepare_data(test_paths)\n",
    "\n",
    "# Filter for balanced classes (6550 samples each for labels 0 and 1)\n",
    "label_0_indices = np.where(y_train == 0)[0][:6550]\n",
    "label_1_indices = np.where(y_train == 1)[0][:6550]\n",
    "balanced_indices = np.concatenate([label_0_indices, label_1_indices])\n",
    "X_train, y_train = X_train[balanced_indices], y_train[balanced_indices]\n",
    "\n",
    "# Shuffle the balanced data\n",
    "X_train, y_train = shuffle(X_train, y_train, random_state=42)\n",
    "\n",
    "# Print the number of segments loaded for verification\n",
    "print(f\"Training segments: {len(X_train)}, Test segments: {len(X_test)}\")\n",
    "\n",
    "# Reshape data for model input format\n",
    "X_train = X_train[..., np.newaxis]  # Adding channel dimension\n",
    "X_test = X_test[..., np.newaxis]\n",
    "\n",
    "\n",
    "print(\"Training data shape:\", X_train.shape)\n",
    "print(\"Training labels shape:\", y_train.shape)\n",
    "print(\"Test data shape:\", X_test.shape)\n",
    "print(\"Test labels shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a994c426-59e4-4a7b-bf20-dbb35dcece8f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: (10451, 1000, 1)\n",
      "Validation data shape: (2613, 1000, 1)\n",
      "Train labels shape: (10451,)\n",
      "Validation labels shape: (2613,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Splitting the training data into train and validation sets with StratifiedKFold\n",
    "strat_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "train_indices, val_indices = next(strat_kfold.split(X_train, y_train))\n",
    "\n",
    "# Creating training and validation sets\n",
    "X_ctrain, X_val = X_train[train_indices], X_train[val_indices]\n",
    "y_ctrain, y_val = y_train[train_indices], y_train[val_indices]\n",
    "\n",
    "# Print shapes of the resulting sets for verification\n",
    "print(\"Train data shape:\", X_ctrain.shape)\n",
    "print(\"Validation data shape:\", X_val.shape)\n",
    "print(\"Train labels shape:\", y_ctrain.shape)\n",
    "print(\"Validation labels shape:\", y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7c624576-e26e-4a11-a86b-ae445d0e4e00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import models, layers, regularizers, optimizers\n",
    "\n",
    "def build_cnn_lstm_model(input_shape):\n",
    "    input_layer = layers.Input(shape=input_shape)\n",
    "    x_bn = layers.BatchNormalization()(input_layer)\n",
    "    \n",
    "    branch1 = layers.Conv1D(24, kernel_size=125, strides=1, activation='relu', padding='same')(x_bn)\n",
    "    branch1 = layers.MaxPooling1D(pool_size=2, strides=1, padding='same')(branch1)\n",
    "    \n",
    "    branch2 = layers.Conv1D(24, kernel_size=15, strides=1, activation='relu', padding='same')(x_bn)\n",
    "    branch2 = layers.MaxPooling1D(pool_size=2, strides=1, padding='same')(branch2)\n",
    "    \n",
    "    branch3 = layers.Conv1D(24, kernel_size=5, strides=1, activation='relu', padding='same')(x_bn)\n",
    "    branch3 = layers.MaxPooling1D(pool_size=2, strides=1, padding='same')(branch3)\n",
    "    \n",
    "    concatenated = layers.Concatenate(axis=-1)([branch1, branch2, branch3])\n",
    "    x = layers.MaxPooling1D(pool_size=3, strides=1, padding='same')(concatenated)\n",
    "    \n",
    "    conv_adjusted = layers.Conv1D(24, kernel_size=3, strides=1, activation='relu', padding='same', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "    x = layers.BatchNormalization()(conv_adjusted)\n",
    "    x = layers.Add()([x, conv_adjusted])\n",
    "    \n",
    "    x = layers.Dense(48, activation='leaky_relu', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    \n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Reshape((1, -1))(x)\n",
    "    x = layers.LSTM(64)(x)\n",
    "    \n",
    "    output_layer = layers.Dense(2, activation='softmax')(x)\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "da14a4f4-9571-4ec0-af55-5f6437a78fbc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)        [(None, 1000, 1)]            0         []                            \n",
      "                                                                                                  \n",
      " batch_normalization_4 (Bat  (None, 1000, 1)              4         ['input_4[0][0]']             \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " conv1d_12 (Conv1D)          (None, 1000, 24)             3024      ['batch_normalization_4[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv1d_13 (Conv1D)          (None, 1000, 24)             384       ['batch_normalization_4[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv1d_14 (Conv1D)          (None, 1000, 24)             144       ['batch_normalization_4[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " max_pooling1d_12 (MaxPooli  (None, 1000, 24)             0         ['conv1d_12[0][0]']           \n",
      " ng1D)                                                                                            \n",
      "                                                                                                  \n",
      " max_pooling1d_13 (MaxPooli  (None, 1000, 24)             0         ['conv1d_13[0][0]']           \n",
      " ng1D)                                                                                            \n",
      "                                                                                                  \n",
      " max_pooling1d_14 (MaxPooli  (None, 1000, 24)             0         ['conv1d_14[0][0]']           \n",
      " ng1D)                                                                                            \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate  (None, 1000, 72)             0         ['max_pooling1d_12[0][0]',    \n",
      " )                                                                   'max_pooling1d_13[0][0]',    \n",
      "                                                                     'max_pooling1d_14[0][0]']    \n",
      "                                                                                                  \n",
      " max_pooling1d_15 (MaxPooli  (None, 1000, 72)             0         ['concatenate_3[0][0]']       \n",
      " ng1D)                                                                                            \n",
      "                                                                                                  \n",
      " conv1d_15 (Conv1D)          (None, 1000, 24)             5208      ['max_pooling1d_15[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_5 (Bat  (None, 1000, 24)             96        ['conv1d_15[0][0]']           \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " add_3 (Add)                 (None, 1000, 24)             0         ['batch_normalization_5[0][0]'\n",
      "                                                                    , 'conv1d_15[0][0]']          \n",
      "                                                                                                  \n",
      " dense_8 (Dense)             (None, 1000, 48)             1200      ['add_3[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)         (None, 1000, 48)             0         ['dense_8[0][0]']             \n",
      "                                                                                                  \n",
      " global_average_pooling1d_3  (None, 48)                   0         ['dropout_3[0][0]']           \n",
      "  (GlobalAveragePooling1D)                                                                        \n",
      "                                                                                                  \n",
      " reshape_3 (Reshape)         (None, 1, 48)                0         ['global_average_pooling1d_3[0\n",
      "                                                                    ][0]']                        \n",
      "                                                                                                  \n",
      " lstm_3 (LSTM)               (None, 64)                   28928     ['reshape_3[0][0]']           \n",
      "                                                                                                  \n",
      " dense_9 (Dense)             (None, 2)                    130       ['lstm_3[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 39118 (152.80 KB)\n",
      "Trainable params: 39068 (152.61 KB)\n",
      "Non-trainable params: 50 (200.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Compile the model with specified learning rate\n",
    "input_shape = (1000, 1)  # Adjust as per 10-second segments at 100 Hz sampling rate\n",
    "model = build_cnn_lstm_model(input_shape)\n",
    "model.compile(optimizer=optimizers.Adam(learning_rate=0.001), \n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Model summary for verification\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "17fb9435-c9ec-4c0a-9cc2-4c7b4392a9b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "163/164 [============================>.] - ETA: 0s - loss: 0.9032 - accuracy: 0.6286\n",
      "Epoch 1: val_loss improved from inf to 0.74089, saving model to C:/Users/abbas/BAU/11Fall 2024/FYP2/apnea-ecg/1.0.0/Model_CheckPoint_draft_3\\best_model.h5\n",
      "164/164 [==============================] - 28s 154ms/step - loss: 0.9027 - accuracy: 0.6287 - val_loss: 0.7409 - val_accuracy: 0.6112 - lr: 0.0010\n",
      "Epoch 2/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abbas\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163/164 [============================>.] - ETA: 0s - loss: 0.5699 - accuracy: 0.7615\n",
      "Epoch 2: val_loss improved from 0.74089 to 0.62962, saving model to C:/Users/abbas/BAU/11Fall 2024/FYP2/apnea-ecg/1.0.0/Model_CheckPoint_draft_3\\best_model.h5\n",
      "164/164 [==============================] - 24s 146ms/step - loss: 0.5702 - accuracy: 0.7612 - val_loss: 0.6296 - val_accuracy: 0.7447 - lr: 0.0010\n",
      "Epoch 3/30\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.5062 - accuracy: 0.7771\n",
      "Epoch 3: val_loss improved from 0.62962 to 0.56244, saving model to C:/Users/abbas/BAU/11Fall 2024/FYP2/apnea-ecg/1.0.0/Model_CheckPoint_draft_3\\best_model.h5\n",
      "164/164 [==============================] - 24s 147ms/step - loss: 0.5062 - accuracy: 0.7771 - val_loss: 0.5624 - val_accuracy: 0.7501 - lr: 0.0010\n",
      "Epoch 4/30\n",
      "163/164 [============================>.] - ETA: 0s - loss: 0.4844 - accuracy: 0.7852\n",
      "Epoch 4: val_loss improved from 0.56244 to 0.50084, saving model to C:/Users/abbas/BAU/11Fall 2024/FYP2/apnea-ecg/1.0.0/Model_CheckPoint_draft_3\\best_model.h5\n",
      "164/164 [==============================] - 24s 147ms/step - loss: 0.4843 - accuracy: 0.7852 - val_loss: 0.5008 - val_accuracy: 0.7811 - lr: 0.0010\n",
      "Epoch 5/30\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.4665 - accuracy: 0.7926\n",
      "Epoch 5: val_loss did not improve from 0.50084\n",
      "164/164 [==============================] - 24s 146ms/step - loss: 0.4665 - accuracy: 0.7926 - val_loss: 0.5249 - val_accuracy: 0.7589 - lr: 0.0010\n",
      "Epoch 6/30\n",
      "163/164 [============================>.] - ETA: 0s - loss: 0.4658 - accuracy: 0.7945\n",
      "Epoch 6: val_loss did not improve from 0.50084\n",
      "164/164 [==============================] - 24s 147ms/step - loss: 0.4658 - accuracy: 0.7946 - val_loss: 0.5476 - val_accuracy: 0.7187 - lr: 0.0010\n",
      "Epoch 7/30\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.4444 - accuracy: 0.8029\n",
      "Epoch 7: val_loss did not improve from 0.50084\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "164/164 [==============================] - 24s 146ms/step - loss: 0.4444 - accuracy: 0.8029 - val_loss: 0.5339 - val_accuracy: 0.7478 - lr: 0.0010\n",
      "Epoch 8/30\n",
      "163/164 [============================>.] - ETA: 0s - loss: 0.4349 - accuracy: 0.8134\n",
      "Epoch 8: val_loss improved from 0.50084 to 0.45194, saving model to C:/Users/abbas/BAU/11Fall 2024/FYP2/apnea-ecg/1.0.0/Model_CheckPoint_draft_3\\best_model.h5\n",
      "164/164 [==============================] - 24s 148ms/step - loss: 0.4347 - accuracy: 0.8134 - val_loss: 0.4519 - val_accuracy: 0.7976 - lr: 5.0000e-04\n",
      "Epoch 9/30\n",
      "163/164 [============================>.] - ETA: 0s - loss: 0.4183 - accuracy: 0.8175\n",
      "Epoch 9: val_loss did not improve from 0.45194\n",
      "164/164 [==============================] - 26s 158ms/step - loss: 0.4182 - accuracy: 0.8175 - val_loss: 0.4802 - val_accuracy: 0.7811 - lr: 5.0000e-04\n",
      "Epoch 10/30\n",
      "163/164 [============================>.] - ETA: 0s - loss: 0.4193 - accuracy: 0.8198\n",
      "Epoch 10: val_loss did not improve from 0.45194\n",
      "164/164 [==============================] - 24s 149ms/step - loss: 0.4191 - accuracy: 0.8198 - val_loss: 0.4586 - val_accuracy: 0.8152 - lr: 5.0000e-04\n",
      "Epoch 11/30\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.4160 - accuracy: 0.8206\n",
      "Epoch 11: val_loss did not improve from 0.45194\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "164/164 [==============================] - 24s 148ms/step - loss: 0.4160 - accuracy: 0.8206 - val_loss: 0.4535 - val_accuracy: 0.8067 - lr: 5.0000e-04\n",
      "Epoch 12/30\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.4024 - accuracy: 0.8292\n",
      "Epoch 12: val_loss did not improve from 0.45194\n",
      "164/164 [==============================] - 24s 149ms/step - loss: 0.4024 - accuracy: 0.8292 - val_loss: 0.4918 - val_accuracy: 0.7723 - lr: 2.5000e-04\n",
      "Epoch 13/30\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.4051 - accuracy: 0.8250\n",
      "Epoch 13: val_loss did not improve from 0.45194\n",
      "164/164 [==============================] - 24s 147ms/step - loss: 0.4051 - accuracy: 0.8250 - val_loss: 0.4531 - val_accuracy: 0.8152 - lr: 2.5000e-04\n",
      "Epoch 14/30\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.3983 - accuracy: 0.8316\n",
      "Epoch 14: val_loss did not improve from 0.45194\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "164/164 [==============================] - 24s 148ms/step - loss: 0.3983 - accuracy: 0.8316 - val_loss: 0.4577 - val_accuracy: 0.8014 - lr: 2.5000e-04\n",
      "Epoch 15/30\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.3920 - accuracy: 0.8330\n",
      "Epoch 15: val_loss improved from 0.45194 to 0.44586, saving model to C:/Users/abbas/BAU/11Fall 2024/FYP2/apnea-ecg/1.0.0/Model_CheckPoint_draft_3\\best_model.h5\n",
      "164/164 [==============================] - 24s 148ms/step - loss: 0.3920 - accuracy: 0.8330 - val_loss: 0.4459 - val_accuracy: 0.8083 - lr: 1.2500e-04\n",
      "Epoch 16/30\n",
      "163/164 [============================>.] - ETA: 0s - loss: 0.3838 - accuracy: 0.8386\n",
      "Epoch 16: val_loss improved from 0.44586 to 0.41870, saving model to C:/Users/abbas/BAU/11Fall 2024/FYP2/apnea-ecg/1.0.0/Model_CheckPoint_draft_3\\best_model.h5\n",
      "164/164 [==============================] - 24s 147ms/step - loss: 0.3838 - accuracy: 0.8385 - val_loss: 0.4187 - val_accuracy: 0.8255 - lr: 1.2500e-04\n",
      "Epoch 17/30\n",
      "163/164 [============================>.] - ETA: 0s - loss: 0.3854 - accuracy: 0.8355\n",
      "Epoch 17: val_loss improved from 0.41870 to 0.41643, saving model to C:/Users/abbas/BAU/11Fall 2024/FYP2/apnea-ecg/1.0.0/Model_CheckPoint_draft_3\\best_model.h5\n",
      "164/164 [==============================] - 24s 147ms/step - loss: 0.3855 - accuracy: 0.8355 - val_loss: 0.4164 - val_accuracy: 0.8213 - lr: 1.2500e-04\n",
      "Epoch 18/30\n",
      "163/164 [============================>.] - ETA: 0s - loss: 0.3845 - accuracy: 0.8379\n",
      "Epoch 18: val_loss did not improve from 0.41643\n",
      "164/164 [==============================] - 24s 147ms/step - loss: 0.3845 - accuracy: 0.8380 - val_loss: 0.4310 - val_accuracy: 0.8263 - lr: 1.2500e-04\n",
      "Epoch 19/30\n",
      "163/164 [============================>.] - ETA: 0s - loss: 0.3811 - accuracy: 0.8379\n",
      "Epoch 19: val_loss did not improve from 0.41643\n",
      "164/164 [==============================] - 24s 148ms/step - loss: 0.3808 - accuracy: 0.8381 - val_loss: 0.4627 - val_accuracy: 0.7976 - lr: 1.2500e-04\n",
      "Epoch 20/30\n",
      "163/164 [============================>.] - ETA: 0s - loss: 0.3859 - accuracy: 0.8353\n",
      "Epoch 20: val_loss did not improve from 0.41643\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "164/164 [==============================] - 24s 149ms/step - loss: 0.3861 - accuracy: 0.8352 - val_loss: 0.4375 - val_accuracy: 0.8201 - lr: 1.2500e-04\n",
      "Epoch 21/30\n",
      "163/164 [============================>.] - ETA: 0s - loss: 0.3789 - accuracy: 0.8425\n",
      "Epoch 21: val_loss did not improve from 0.41643\n",
      "164/164 [==============================] - 24s 149ms/step - loss: 0.3789 - accuracy: 0.8424 - val_loss: 0.4353 - val_accuracy: 0.8201 - lr: 6.2500e-05\n",
      "Epoch 22/30\n",
      "163/164 [============================>.] - ETA: 0s - loss: 0.3756 - accuracy: 0.8438\n",
      "Epoch 22: val_loss improved from 0.41643 to 0.41196, saving model to C:/Users/abbas/BAU/11Fall 2024/FYP2/apnea-ecg/1.0.0/Model_CheckPoint_draft_3\\best_model.h5\n",
      "164/164 [==============================] - 27s 166ms/step - loss: 0.3758 - accuracy: 0.8436 - val_loss: 0.4120 - val_accuracy: 0.8301 - lr: 6.2500e-05\n",
      "Epoch 23/30\n",
      "163/164 [============================>.] - ETA: 0s - loss: 0.3758 - accuracy: 0.8437\n",
      "Epoch 23: val_loss did not improve from 0.41196\n",
      "164/164 [==============================] - 24s 144ms/step - loss: 0.3759 - accuracy: 0.8437 - val_loss: 0.4299 - val_accuracy: 0.8213 - lr: 6.2500e-05\n",
      "Epoch 24/30\n",
      "163/164 [============================>.] - ETA: 0s - loss: 0.3779 - accuracy: 0.8414\n",
      "Epoch 24: val_loss did not improve from 0.41196\n",
      "164/164 [==============================] - 24s 145ms/step - loss: 0.3777 - accuracy: 0.8415 - val_loss: 0.4406 - val_accuracy: 0.8086 - lr: 6.2500e-05\n",
      "Epoch 25/30\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.3760 - accuracy: 0.8415\n",
      "Epoch 25: val_loss did not improve from 0.41196\n",
      "\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "164/164 [==============================] - 28s 169ms/step - loss: 0.3760 - accuracy: 0.8415 - val_loss: 0.4385 - val_accuracy: 0.8106 - lr: 6.2500e-05\n",
      "Epoch 26/30\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.3698 - accuracy: 0.8470\n",
      "Epoch 26: val_loss did not improve from 0.41196\n",
      "164/164 [==============================] - 26s 160ms/step - loss: 0.3698 - accuracy: 0.8470 - val_loss: 0.4313 - val_accuracy: 0.8194 - lr: 3.1250e-05\n",
      "Epoch 27/30\n",
      "163/164 [============================>.] - ETA: 0s - loss: 0.3699 - accuracy: 0.8442\n",
      "Epoch 27: val_loss did not improve from 0.41196\n",
      "164/164 [==============================] - 26s 158ms/step - loss: 0.3696 - accuracy: 0.8444 - val_loss: 0.4243 - val_accuracy: 0.8289 - lr: 3.1250e-05\n",
      "Epoch 28/30\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.3713 - accuracy: 0.8433\n",
      "Epoch 28: val_loss did not improve from 0.41196\n",
      "\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "164/164 [==============================] - 26s 159ms/step - loss: 0.3713 - accuracy: 0.8433 - val_loss: 0.4233 - val_accuracy: 0.8228 - lr: 3.1250e-05\n",
      "Epoch 29/30\n",
      "163/164 [============================>.] - ETA: 0s - loss: 0.3682 - accuracy: 0.8442\n",
      "Epoch 29: val_loss did not improve from 0.41196\n",
      "164/164 [==============================] - 26s 159ms/step - loss: 0.3687 - accuracy: 0.8440 - val_loss: 0.4179 - val_accuracy: 0.8316 - lr: 1.5625e-05\n"
     ]
    }
   ],
   "source": [
    "# Callbacks\n",
    "checkpoint_path = \"best_model.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath=checkpoint_path, monitor='val_loss', save_best_only=True, mode='min', verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6, verbose=1)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True)\n",
    "\n",
    "# Class weights\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=[0, 1], y=y_ctrain)\n",
    "class_weights_dict = {0: class_weights[0], 1: class_weights[1]}\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_ctrain, y_ctrain, \n",
    "                    validation_data=(X_val, y_val), \n",
    "                    epochs=30, \n",
    "                    batch_size=64,\n",
    "                    class_weight=class_weights_dict,\n",
    "                    callbacks=[checkpoint, reduce_lr, early_stopping]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c7dcdd2c-e97e-4fdf-88ef-f41b6678ff23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 57.68%\n",
      "540/540 [==============================] - 10s 18ms/step\n",
      "Total Accuracy (TAC): 57.68%\n",
      "Sensitivity (SE): 76.69%\n",
      "Specificity (SP): 46.06%\n",
      "Positive Predictive Value (PPV): 46.49%\n",
      "Negative Predictive Value (NPV): 76.38%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, cohen_kappa_score\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Predict on the test set\n",
    "y_test_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "\n",
    "# Compute confusion matrix and evaluation metrics\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_test_pred).ravel()\n",
    "TAC = (tp + tn) / (tp + tn + fp + fn) * 100\n",
    "sensitivity = tp / (tp + fn) * 100\n",
    "specificity = tn / (tn + fp) * 100\n",
    "PPV = tp / (tp + fp) * 100\n",
    "NPV = tn / (tn + fn) * 100\n",
    "kappa = cohen_kappa_score(y_test, y_test_pred)\n",
    "\n",
    "# Display metrics\n",
    "print(f\"Total Accuracy (TAC): {TAC:.2f}%\")\n",
    "print(f\"Sensitivity (SE): {sensitivity:.2f}%\")\n",
    "print(f\"Specificity (SP): {specificity:.2f}%\")\n",
    "print(f\"Positive Predictive Value (PPV): {PPV:.2f}%\")\n",
    "print(f\"Negative Predictive Value (NPV): {NPV:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967bb32e-7907-47df-ba7d-2527c4584862",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
